# run_all_models.py
# Requirements:
#   - models_list.txt (created by discover_models_flat.py, full model paths, one per line)
#   - modelloop.json (workflow exported from ComfyUI, using Tea_CheckpointFromPath)
# Usage:
#   1. Start ComfyUI
#   2. Run:  python run_all_models.py


from pathlib import Path
import json, time, traceback, requests

# --- adjust if you named things differently ---
COMFY        = "http://127.0.0.1:8188"
TEMPLATE     = "modelloop.json"     # your workflow saved in API format, using Tea_CheckpointFromPath
MODELS_TXT   = "models_list.txt"    # generated by discover_models_flat.py (full paths, one per line)
LOADER_CLASS = "Tea_CheckpointFromPath"
SAVE_CLASSES = {"SaveImage"}        # works with stock SaveImage;

# -------- helpers --------
def nowstamp():
    return time.strftime("%Y%m%d-%H%M%S")

def deep_copy(d):
    return json.loads(json.dumps(d))

def read_lines(path: Path):
    return [ln.strip() for ln in path.read_text(encoding="utf-8").splitlines()
            if ln.strip() and not ln.strip().startswith("#")]

def load_prompt(path: Path) -> dict:
    return json.loads(path.read_text(encoding="utf-8"))

def set_ckpt_path(prompt: dict, model_path: str) -> bool:
    found = False
    nodes = prompt.get("nodes", {})
    for _, node in nodes.items():
        if node.get("class_type") == LOADER_CLASS:
            node.setdefault("inputs", {})["ckpt_path"] = model_path
            found = True
    return found

def set_filename_prefix(prompt: dict, prefix: str) -> bool:
    """Set filename_prefix on SaveImage (widgets_values[0]). Returns True if at least one changed."""
    changed = False
    nodes = prompt.get("nodes", {})
    for _, node in nodes.items():
        cls = node.get("class_type")
        if cls in SAVE_CLASSES:
            w = node.setdefault("widgets_values", [])
            if not w:
                node["widgets_values"] = [prefix]
            else:
                # widgets_values[0] is filename_prefix on stock SaveImage
                node["widgets_values"][0] = prefix
            changed = True
        # Optional: if your save node has a 'subfolder' input, set it too
        inputs = node.get("inputs", {})
        if "subfolder" in inputs:
            inputs["subfolder"] = prefix.split("/")[0]  # put run folder in subfolder, keep filename prefix short
    return changed

def enqueue(prompt: dict) -> str:
    r = requests.post(f"{COMFY}/prompt", json={"prompt": prompt})
    r.raise_for_status()
    return r.json().get("prompt_id", "")

def wait_until_idle(poll_s=1.2):
    while True:
        q = requests.get(f"{COMFY}/queue").json()
        if q.get("pending", 0) == 0 and q.get("running", 0) == 0:
            return
        time.sleep(poll_s)

# -------- main --------
def main():
    here = Path(__file__).parent
    tpl_path = here / TEMPLATE
    list_path = here / MODELS_TXT

    if not tpl_path.exists():
        raise FileNotFoundError(f"Template not found: {tpl_path}")
    if not list_path.exists():
        raise FileNotFoundError(f"Model list not found: {list_path}")

    models = read_lines(list_path)
    if not models:
        print("! models_list.txt is empty.")
        return

    run = nowstamp()
    ran_log   = here / f"models_ran_{run}.txt"
    fail_log  = here / f"failed_models_{run}.txt"

    tpl = load_prompt(tpl_path)
    ran = []
    failed = []

    print(f"Queuing {len(models)} models. Run folder: {run}")
    for m in models:
        model_path = Path(m)
        stem = model_path.stem
        # Prefix includes a subfolder = run timestamp, and model stem
        # e.g. outputs/<run>/<stem>__00001.png
        prefix = f"{run}/{stem}__"
        print(f"\n=== {stem} ===")

        try:
            prompt = deep_copy(tpl)
            if not set_ckpt_path(prompt, str(model_path)):
                print("! Loader node not found in template; skipping.")
                failed.append(m)
                continue
            set_filename_prefix(prompt, prefix)
            pid = enqueue(prompt)
            print(f"queued prompt_id={pid}  prefix='{prefix}'")
            wait_until_idle()
            print("✓ done")
            ran.append(m)
        except requests.HTTPError as e:
            print(f"! HTTP error on {stem}: {e}")
            traceback.print_exc()
            failed.append(m)
        except Exception as e:
            print(f"! failed on {stem}: {e}")
            traceback.print_exc()
            failed.append(m)

    # logs
    if ran:
        ran_log.write_text("\n".join(ran), encoding="utf-8")
        print(f"\nSaved ran list → {ran_log}")
    if failed:
        fail_log.write_text("\n".join(failed), encoding="utf-8")
        print(f"Some failed; saved list → {fail_log}")

if __name__ == "__main__":
    main()
